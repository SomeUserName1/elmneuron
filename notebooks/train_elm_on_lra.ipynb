{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Training ELM on Long Range Arena (LRA)\n",
    "\n",
    "This notebook demonstrates how to train an ELM model on the Long Range Arena benchmark using PyTorch Lightning.\n",
    "\n",
    "LRA is a benchmark for evaluating long-context sequence models across 5 diverse tasks:\n",
    "- **ListOps** (2K): Hierarchical mathematical reasoning\n",
    "- **Text** (4K): IMDb sentiment analysis (byte-level)\n",
    "- **Retrieval** (8K): Document matching\n",
    "- **Image** (1K): CIFAR-10 as sequences\n",
    "- **Pathfinder** (1K): Visual path connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elm-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ELM components\n",
    "from elmneuron.expressive_leaky_memory_neuron_v2 import ELM\n",
    "from elmneuron.tasks.classification_task import ClassificationTask\n",
    "from elmneuron.lra.lra_datamodule import (\n",
    "    ListOpsDataModule,\n",
    "    LRATextDataModule,\n",
    "    LRARetrievalDataModule,\n",
    "    LRAImageDataModule,\n",
    "    LRAPathfinderDataModule,\n",
    ")\n",
    "from elmneuron.callbacks import (\n",
    "    SequenceVisualizationCallback,\n",
    "    MemoryDynamicsCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding & Config\n",
    "general_seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(general_seed)\n",
    "random.seed(general_seed)\n",
    "np.random.seed(general_seed)\n",
    "torch.manual_seed(general_seed)\n",
    "torch.cuda.manual_seed(general_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Dataset config\n",
    "lra_task = \"listops\"  # Choose: \"listops\", \"text\", \"retrieval\", \"image\", \"pathfinder\"\n",
    "data_dir = \"./data/lra\"\n",
    "batch_size = 32  # Smaller batch size for long sequences\n",
    "num_workers = 4\n",
    "\n",
    "# Model config (adjust based on task)\n",
    "num_memory = 200  # Higher for longer sequences\n",
    "lambda_value = 5.0\n",
    "tau_b_value = 1.0\n",
    "memory_tau_min = 1.0\n",
    "memory_tau_max = 500.0  # Higher for long-range dependencies\n",
    "learn_memory_tau = False\n",
    "\n",
    "# Training config\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50  # LRA tasks may require more epochs\n",
    "\n",
    "print(f\"Training ELM on LRA task: {lra_task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datamodule-header",
   "metadata": {},
   "source": [
    "## Setup DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "datamodule",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataModule based on selected task\n",
    "if lra_task == \"listops\":\n",
    "    datamodule = ListOpsDataModule(\n",
    "        data_dir=data_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    seq_info = \"2048 tokens, 10 classes\"\n",
    "    \n",
    "elif lra_task == \"text\":\n",
    "    datamodule = LRATextDataModule(\n",
    "        data_dir=data_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    seq_info = \"4096 bytes, 2 classes (sentiment)\"\n",
    "    \n",
    "elif lra_task == \"retrieval\":\n",
    "    datamodule = LRARetrievalDataModule(\n",
    "        data_dir=data_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    seq_info = \"8192 tokens (2 docs), 2 classes (match/no-match)\"\n",
    "    \n",
    "elif lra_task == \"image\":\n",
    "    datamodule = LRAImageDataModule(\n",
    "        data_dir=data_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    seq_info = \"1024 pixels (32x32), 10 classes (CIFAR-10)\"\n",
    "    \n",
    "elif lra_task == \"pathfinder\":\n",
    "    difficulty = \"easy\"  # Choose: \"easy\", \"medium\", \"hard\"\n",
    "    datamodule = LRAPathfinderDataModule(\n",
    "        data_dir=data_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        difficulty=difficulty,\n",
    "    )\n",
    "    seq_info = f\"1024 pixels (32x32), 2 classes (connected/not), difficulty: {difficulty}\"\n",
    "\n",
    "# Prepare and setup data\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "print(f\"Task: {lra_task}\")\n",
    "print(f\"Sequence info: {seq_info}\")\n",
    "print(f\"Input dimension: {datamodule.input_dim}\")\n",
    "print(f\"Number of classes: {datamodule.num_classes}\")\n",
    "print(f\"Training samples: {len(datamodule.train_dataset) if hasattr(datamodule, 'train_dataset') else 'Loading...'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base ELM model\n",
    "elm_model = ELM(\n",
    "    num_input=datamodule.input_dim,\n",
    "    num_output=datamodule.num_classes,\n",
    "    num_memory=num_memory,\n",
    "    lambda_value=lambda_value,\n",
    "    tau_b_value=tau_b_value,\n",
    "    memory_tau_min=memory_tau_min,\n",
    "    memory_tau_max=memory_tau_max,\n",
    "    learn_memory_tau=learn_memory_tau,\n",
    ")\n",
    "\n",
    "# Wrap in Lightning classification task\n",
    "lightning_module = ClassificationTask(\n",
    "    model=elm_model,\n",
    "    learning_rate=learning_rate,\n",
    "    optimizer=\"adam\",\n",
    "    scheduler=\"cosine\",\n",
    "    scheduler_kwargs={\"T_max\": num_epochs * 1000},  # Approximate\n",
    "    output_selection=\"last\",  # Use last timestep for classification\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in elm_model.parameters())\n",
    "print(f\"Model initialized with {num_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    # Model checkpointing\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"./checkpoints_lra_{lra_task}\",\n",
    "        filename=f\"elm-lra-{lra_task}-{{epoch:02d}}-{{val/accuracy:.4f}}\",\n",
    "        monitor=\"val/accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=3,\n",
    "        save_last=True,\n",
    "    ),\n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor=\"val/accuracy\",\n",
    "        patience=10,  # Higher patience for LRA\n",
    "        mode=\"max\",\n",
    "        verbose=True,\n",
    "    ),\n",
    "    # Visualization callbacks (careful with long sequences)\n",
    "    SequenceVisualizationCallback(\n",
    "        log_every_n_epochs=10,\n",
    "        num_samples=2,  # Fewer samples for long sequences\n",
    "        task_type=\"classification\",\n",
    "        save_dir=f\"./visualizations_lra_{lra_task}\",\n",
    "        log_to_wandb=False,\n",
    "    ),\n",
    "    MemoryDynamicsCallback(\n",
    "        log_every_n_epochs=10,\n",
    "        num_samples=2,\n",
    "        save_dir=f\"./memory_lra_{lra_task}\",\n",
    "        log_to_wandb=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=callbacks,\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=50,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0,\n",
    "    # Consider using gradient accumulation for long sequences\n",
    "    accumulate_grad_batches=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"Note: LRA tasks may take significant time due to long sequences\")\n",
    "trainer.fit(lightning_module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"Testing model...\")\n",
    "test_results = trainer.test(lightning_module, datamodule=datamodule, ckpt_path=\"best\")\n",
    "\n",
    "test_accuracy = test_results[0]['test/accuracy']\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_path = f\"./lra_{lra_task}_best_model.pt\"\n",
    "torch.save(lightning_module.model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes-header",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### LRA Benchmark Tasks\n",
    "\n",
    "| Task | Sequence Length | Classes | Description | Expected Baseline |\n",
    "|------|----------------|---------|-------------|-------------------|\n",
    "| ListOps | 2,048 | 10 | Hierarchical mathematical operations | ~35-40% |\n",
    "| Text | 4,096 | 2 | IMDb sentiment (byte-level) | ~60-65% |\n",
    "| Retrieval | 8,192 | 2 | Document matching | ~55-60% |\n",
    "| Image | 1,024 | 10 | CIFAR-10 as sequences | ~40-45% |\n",
    "| Pathfinder | 1,024 | 2 | Visual path connectivity | ~60-70% |\n",
    "\n",
    "### Training Tips for LRA\n",
    "\n",
    "1. **Memory Efficiency**:\n",
    "   - Use smaller batch sizes (16-32)\n",
    "   - Consider gradient accumulation\n",
    "   - Enable mixed precision training\n",
    "\n",
    "2. **Hyperparameters**:\n",
    "   - Increase `num_memory` for longer sequences (200-500)\n",
    "   - Increase `memory_tau_max` for long-range dependencies (500-1000)\n",
    "   - Use gradient clipping (1.0)\n",
    "\n",
    "3. **Training Time**:\n",
    "   - LRA tasks are computationally expensive\n",
    "   - Consider using GPU acceleration\n",
    "   - Each epoch may take 10-30 minutes\n",
    "\n",
    "### Pathfinder Difficulty Levels\n",
    "\n",
    "The Pathfinder task has three difficulty levels:\n",
    "- **Easy**: Shorter paths, clearer connections\n",
    "- **Medium**: Moderate complexity\n",
    "- **Hard**: Longer paths, more distractors\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "Compare your results with published baselines:\n",
    "- Transformers: ~35-60% depending on task\n",
    "- S4 (State Space Models): ~60-80%\n",
    "- ELM: Results will vary based on hyperparameters\n",
    "\n",
    "### Running All Tasks\n",
    "\n",
    "To benchmark ELM across all LRA tasks:\n",
    "\n",
    "```python\n",
    "tasks = [\"listops\", \"text\", \"retrieval\", \"image\", \"pathfinder\"]\n",
    "for task in tasks:\n",
    "    # Update lra_task variable and run training\n",
    "    print(f\"Training on {task}...\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
